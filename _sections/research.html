---
title: Research Interest
icon: fa-lightbulb
order: 4
---

<a href="assets/images/research-teaser.jpeg" class="image featured"><img src="assets/images/research-teaser.jpeg" alt="" title="synthetic point cloud with point-wise semantic segmentation and object bounding box annotations" /></a>

<p>
    I conduct my research at Mercedes-Benz AG in Stuttgart as part of my PhD studies at Karlsruhe Institute of Technology.
    The thesis is in the field of deep generative models and perception on 3D data, more precisely LiDAR point clouds.
    It can be narrowed down to a series of domain adaptation challenges in the world of autonomous vehicles.
    Typical use cases are robustness to environmental changes and unexpected and rare events.
    This is a big challenge for traditional deep learning based perception models.
</p>
<p>
    My first aim is to make these systems more robust to changes in their environment, for example by using simulated data (see picture).
    The big advantage of simulation is that we can generate an almost endless amount of data with precise annotations and all desired edge cases.
    However, trained solely on this data, a model performs poorly in the real world.
    One possible solution is to close the appearance gap between two worlds prior to training a network on this data.
    This is what I am currently working on.
</p>
<p>
    Though, the real challenge here is to make a profound statement about the quality of the generated samples.
    A lot of solutions emerged for this problem, however most approaches are either impractical or not applicable to 3D data.
    Therefore, my second aim is to design a reliable metric that can quantify the quality of a 3D point cloud in terms of realism and domain allegiance.
</p>
<p>
    I am further interested in all kinds of machine learning applications, especially in the computer vision and environment perception area.
    For the future, I also want to learn about reinforcement learning.
    If you are interested in my research, feel free to contact me.
</p>
