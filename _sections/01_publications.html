---
title: Publications
order: 1
---

<div class="d-flex flex-column flex-md-row justify-content-between mb-2">
    <div class="flex-grow-1">
        <h3>Papers</h3>
    </div>
</div>

<div id="paper_scan-semseg" class="d-flex flex-column flex-md-row justify-content-between mb-5">
    <div class="flex-grow-1">
        <div class="subheading mb-0"><span class="text-primary">A Survey on Deep Domain Adaptation for LiDAR Perception</span></div>
        <div class="mb-2">IEEE Intelligent Vehicles Symposium (IV) 2021, Workshop Autonomy@Scale</div>
        <div class="mb-3">Larissa T. Triess, Mariella Dreissig, Christoph B. Rist, J. Marius Zöllner</div>
        <div class="mb-3">
            Scalable systems for automated driving have toreliably cope with an open-world setting.
            This means, theperception systems are exposed to drastic domain shifts, likechanges in weather conditions, time-dependent aspects, or geographic regions.
            Covering all domains with annotated data is impossible because of the endless variations of domains and the time-consuming and expensive annotation process.
            Furthermore, fast development cycles of the system additionally introduce hardware changes, such as sensor types and vehicle setups, and the required knowledge transfer from simulation.

            To enable scalable automated driving, it is therefore crucial to address these domain shifts in a robust and efficient manner.
            Over the last years, a vast amount of different domain adaptation techniques evolved.
            There already exists a number of survey papers for domain adaptation on camera images, however, a survey for LiDAR perception is absent.
            Nevertheless, LiDAR is a vital sensor for automated driving that provides detailed 3D scans of the vehicle’s surroundings.
            To stimulate future research, this paper presents a comprehensive review of recent progress in domain adaptation methods and formulates interesting research questions specifically targeted towards LiDAR perception.
        </div>

        <div style="text-align: center; align-content: center; width: 50%">
            <div class="row mb-5">
                <div class="col row-cols-1">
                    <a href="/assets/img/sections/teaser_survey.svg">
                        <img class="img-fluid img-centered img-zoom mx-auto" src="/assets/img/sections/teaser_survey.svg" alt="icon-uda-survey"/>
                    </a>
                </div>
            </div>
        </div>

        <a target=_blank href=https://arxiv.org/pdf/2106.02377.pdf class="btn btn-dark"><i class="fas fa-file-pdf" aria-hidden=true></i>&nbsp;pdf</a>
        <a target=_blank href=https://larissa.triess.eu/survey-uda-lidar class="btn btn-light"><i class="fa fa-rocket" aria-hidden=true></i>&nbsp;project</a>
        <button class="btn btn-light" onclick="document.getElementById('idtriess2021ivwork').style.display='block'"><i class="fa fa-quote-left" aria-hidden=true></i>&nbsp;cite</button>
        <div id="idtriess2021ivwork" class="modal">
            <div class="modal-dialog-centered">
                <div class="modal-content">
                    <p id="bibtex_triess2021ivwork">
                        @inproceedings{triess2021iv,<br>
                            &emsp;title={A Survey on Deep Domain Adaptation for LiDAR Perception},<br>
                            &emsp;author={Triess, Larissa T. and Dreissig, Mariella and Rist, Christoph B. and Z\"ollner, J. Marius},<br>
                            &emsp;booktitle={Proc. IEEE Intelligent Vehicles Symposium (IV) Workshops},<br>
                            &emsp;year={2021}<br>
                        }
                    </p>

                    <div style="text-align: center;">
                        <div class="row">
                            <div class="col row-cols-1">
                                <button class="btn btn-outline-primary" style="width: 100%;" onclick="copyToClipboard('#bibtex_triess2021ivwork')"><i class="far fa-copy"></i>&nbsp;copy</button>
                            </div>
                            <div class="col row-cols-1">
                                <button class="btn btn-outline-dark" style="width: 100%;" onclick="document.getElementById('idtriess2021ivwork').style.display='none'"><i class="fas fa-times"></i>&nbsp;close</button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<div id="paper_scan-semseg" class="d-flex flex-column flex-md-row justify-content-between mb-5">
    <div class="flex-grow-1">
        <div class="subheading mb-0"><span class="text-primary">Scan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental Study</span></div>
        <div class="mb-2">IEEE Intelligent Vehicles Symposium (IV) 2020</div>
        <div class="mb-3">Larissa T. Triess, David Peter, Christoph B. Rist, J. Marius Zöllner</div>
        <div class="mb-3">
            Autonomous vehicles need to have a semantic understanding of the three-dimensional world around them in order to reason about their environment.
            State of the art methods use deep neural networks to predict semantic classes for each point in a LiDAR scan.
            A powerful and efficient way to process LiDAR measurements is to use two-dimensional, image- like projections.
            In this work, we perform a comprehensive experimental study of image-based semantic segmentation architectures for LiDAR point clouds.
            We demonstrate various techniques to boost the performance and to improve runtime as well as memory constraints.
        </div>

        <div style="text-align: center;">
            <div class="row mb-5">
                <div class="col row-cols-1">
                    <a href="/assets/img/sections/teaser_scan-semseg_01.svg">
                        <img class="img-fluid img-centered img-zoom mx-auto" src="/assets/img/sections/teaser_scan-semseg_01.svg" alt="icon-networksize"/>
                    </a>
                </div>
                <div class="col row-cols-1">
                    <a href="/assets/img/sections/teaser_scan-semseg_02.svg">
                        <img class="img-fluid img-centered img-zoom mx-auto" src="/assets/img/sections/teaser_scan-semseg_02.svg" alt="icon-scanunfolding"/>
                    </a>
                </div>
                <div class="col row-cols-1">
                    <a href="/assets/img/sections/teaser_scan-semseg_03.svg">
                        <img class="img-fluid img-centered img-zoom mx-auto" src="/assets/img/sections/teaser_scan-semseg_03.svg" alt="icon-lossfunction"/>
                    </a>
                </div>
                <div class="col row-cols-1">
                    <a href="/assets/img/sections/teaser_scan-semseg_04.svg">
                        <img class="img-fluid img-centered img-zoom mx-auto" src="/assets/img/sections/teaser_scan-semseg_04.svg" alt="icon-slc"/>
                    </a>
                </div>
            </div>
        </div>

        <a target=_blank href=https://arxiv.org/pdf/2004.11803.pdf class="btn btn-dark"><i class="fas fa-file-pdf" aria-hidden=true></i>&nbsp;pdf</a>
        <a target=_blank href=https://larissa.triess.eu/scan-semseg class="btn btn-light"><i class="fa fa-rocket" aria-hidden=true></i>&nbsp;project</a>
        <a target=_blank href=https://larissa.triess.eu/scan-semseg#code class="btn btn-light"><i class="fa fa-code" aria-hidden=true></i>&nbsp;code</a>
        <button class="btn btn-light" onclick="document.getElementById('idtriess2020arxiv').style.display='block'"><i class="fa fa-quote-left" aria-hidden=true></i>&nbsp;cite</button>
        <div id="idtriess2020arxiv" class="modal">
            <div class="modal-dialog-centered">
                <div class="modal-content">
                    <p id="bibtex_triess2020arxiv">
                        @article{triess2020arxiv,<br>
                            &emsp;title={Scan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental Study},<br>
                            &emsp;author={Triess, Larissa T. and Peter, David and Rist, Christoph B. and Z\"ollner, J. Marius},<br>
                            &emsp;journal={arxiv},<br>
                            &emsp;year={2020}<br>
                        }
                    </p>

                    <div style="text-align: center;">
                        <div class="row">
                            <div class="col row-cols-1">
                                <button class="btn btn-outline-primary" style="width: 100%;" onclick="copyToClipboard('#bibtex_triess2020arxiv')"><i class="far fa-copy"></i>&nbsp;copy</button>
                            </div>
                            <div class="col row-cols-1">
                                <button class="btn btn-outline-dark" style="width: 100%;" onclick="document.getElementById('idtriess2020arxiv').style.display='none'"><i class="fas fa-times"></i>&nbsp;close</button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<div id="paper_pc-upsampling" class="d-flex flex-column flex-md-row justify-content-between mb-5">
    <div class="flex-grow-1">
        <div class="subheading mb-0"><span class="text-primary">CNN-based synthesis of realistic high-resolution LiDAR data</span></div>
        <div class="mb-2">IEEE Intelligent Vehicles Symposium (IV) 2019</div>
        <div class="mb-3">Larissa T. Triess, David Peter, Christoph B. Rist, Markus Enzweiler, J. Marius Zöllner</div>
        <div class="mb-3">
            This paper presents a novel CNN-based approach for synthesizing high-resolution LiDAR point cloud data.
            Our approach generates semantically and perceptually realistic results with guidance from specialized loss-functions.
            First, we utilize a modified per-point loss that addresses missing LiDAR point measurements.
            Second, we align the quality of our generated output with real-world sensor data by applying a perceptual loss.
            <br>
            In large-scale experiments on real-world datasets, we evaluate both the geometric accuracy and semantic segmentation performance using our generated data vs. ground truth.
            In a mean opinion score testing we further assess the perceptual quality of our generated point clouds.
            Our results demonstrate a significant quantitative and qualitative improvement in both geometry and semantics over traditional non CNN-based up-sampling methods
        </div>

        <a href="/assets/img/sections/teaser_pc-upsampling.png">
            <div class="mb-5"><img class="img-fluid img-centered img-zoom mx-auto mb-2" src="/assets/img/sections/teaser_pc-upsampling.png" alt="" /></div>
        </a>

        <a target=_blank href=https://arxiv.org/pdf/1907.00787.pdf class="btn btn-dark"><i class="fas fa-file-pdf" aria-hidden=true></i>&nbsp;pdf</a>
        <a target=_blank href="/assets/pdf/triess2019iv_poster.pdf" class="btn btn-light"><i class="fas fa-map" aria-hidden=true></i>&nbsp;poster</a>
        <a target=_blank href=https://larissa.triess.eu/pc-upsampling class="btn btn-light"><i class="fa fa-rocket" aria-hidden=true></i>&nbsp;project</a>
        <button class="btn btn-light" onclick="document.getElementById('idtriess2019iv').style.display='block'"><i class="fa fa-quote-left" aria-hidden=true></i>&nbsp;cite</button>
        <div id="idtriess2019iv" class="modal">
            <div class="modal-dialog-centered">
                <div class="modal-content">
                    <p id="bibtex_triess2019iv">
                        @inproceedings{triess2019iv,<br>
                            &emsp;author={Triess, Larissa T. and Peter, David and Rist, Christoph B. and Enzweiler, Markus and Z\"ollner, J. Marius},<br>
                            &emsp;booktitle={2019 IEEE Intelligent Vehicles Symposium (IV)},<br>
                            &emsp;title={CNN-based synthesis of realistic high-resolution LiDAR data},<br>
                            &emsp;year={2019},<br>
                            &emsp;pages={1512-1519},<br>
                        }
                    </p>
                    <div style="text-align: center;">
                        <div class="row">
                            <div class="col row-cols-1">
                                <button class="btn btn-outline-primary" style="width: 100%;" onclick="copyToClipboard('#bibtex_triess2019iv')"><i class="far fa-copy"></i>&nbsp;copy</button>
                            </div>
                            <div class="col row-cols-1">
                                <button class="btn btn-outline-dark" style="width: 100%;" onclick="document.getElementById('idtriess2019iv').style.display='none'"><i class="fas fa-times"></i>&nbsp;close</button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<div class="d-flex flex-column flex-md-row justify-content-between mb-2">
    <div class="flex-grow-1">
        <h3>Patents</h3>
    </div>
</div>

<div class="d-flex flex-column flex-md-row justify-content-between">
    <div class="flex-grow-1">
        <div class="subheading mb-0">DE102021002684</div>
        <div><u>L.T. Triess</u>, C.B. Rist. 2021. Method for semantic segmentation of first sensor data of a first sensor type.</div>
        <div>German Patent DE102021002684. Filed 21.05.2021. <i>Patent pending.</i></div>
    </div>
</div>

<div class="d-flex flex-column flex-md-row justify-content-between">
    <div class="flex-grow-1">
        <div class="subheading mb-0">DE102021002689</div>
        <div><u>L.T. Triess</u>, D. Peter. 2021. Method for transforming sensor data.</div>
        <div>German Patent DE102021002689. Filed 21.05.2021. <i>Patent pending.</i></div>
    </div>
</div>

<div class="d-flex flex-column flex-md-row justify-content-between">
    <div class="flex-grow-1">
        <div class="subheading mb-0">DE102021002559</div>
        <div><u>L.T. Triess</u>, D. Peter. 2021. Method for generating realistic maps of beam outages in simulated LiDAR data.</div>
        <div>German Patent DE102021002559. Filed 17.05.2021. <i>Patent pending.</i></div>
    </div>
</div>

<div class="d-flex flex-column flex-md-row justify-content-between">
    <div class="flex-grow-1">
        <div class="subheading mb-0"><a href="https://patents.google.com/patent/DE102021001043A1/en">DE102021001043A1</a></div>
        <div><u>L.T. Triess</u>, D. Peter. 2021. Method for automatic detection and localization of anomalies in data acquired by means of a LiDAR sensor.</div>
        <div>German Patent DE102021001043A1. Filed 26.02.2021. Issued 15.04.2021.</div>
    </div>
</div>

<div class="d-flex flex-column flex-md-row justify-content-between">
    <div class="flex-grow-1">
        <div class="subheading mb-0"><a href="https://patents.google.com/patent/DE102021000803A1/en">DE102021000803A1</a></div>
        <div><u>L.T. Triess</u>, D. Peter. 2021. Method for training a neural network of an electronic computing device of a motor vehicle.</div>
        <div>German Patent DE102021000803A1. Filed 16.02.2021. Issued 15.04.2021.</div>
    </div>
</div>

<div class="d-flex flex-column flex-md-row justify-content-between">
    <div class="flex-grow-1">
        <div class="subheading mb-0"><a href="https://patents.google.com/patent/DE102020001541A1/en">DE102020001541A1</a></div>
        <div><u>L.T. Triess</u>. 2020. Method for transforming acquired sensor data from a first data domain into a second data domain.</div>
        <div>German Patent DE102020001541A1. Filed 09.03.2020. Issued 01.10.2020.</div>
    </div>
</div>

<div class="d-flex flex-column flex-md-row justify-content-between mb-2">
    <div class="flex-grow-1">
        <div class="subheading mb-0"><a href="https://patents.google.com/patent/DE102019003621A1/en">DE102019003621A1</a></div>
        <div><u>L.T. Triess</u>, D. Peter. 2020. Method for processing LiDAR sensor data.</div>
        <div>German Patent DE102019003621A1. Filed 23.05.2019. Issued 02.01.2020.</div>
    </div>
</div>
